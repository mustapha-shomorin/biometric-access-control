{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb33b0d",
   "metadata": {},
   "source": [
    "##### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "456d8284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Portfolio\\GitHub\\biometric_access_control\\venv310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import queue\n",
    "import threading\n",
    "import shutil\n",
    "import sounddevice as sd\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import scipy.io.wavfile as wav\n",
    "from scipy.io.wavfile import write\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from datetime import datetime\n",
    "from playsound import playsound\n",
    "from speechbrain.inference.speaker import SpeakerRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e55490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sounddevice as sd\n",
    "import scipy.io.wavfile as wav\n",
    "from speechbrain.pretrained import SpeakerRecognition\n",
    "\n",
    "# --- CONFIGURATIONS ---\n",
    "FACE_MATCH_THRESHOLD = 0.6\n",
    "VOICE_MATCH_THRESHOLD = 0.75\n",
    "KNOWN_FACES_DIR = \"faces\"\n",
    "VOICE_EMBEDDINGS_DIR = \"voice_embeddings\"\n",
    "ACCESS_LOG_FILE = \"access_log.txt\"\n",
    "\n",
    "# --- INITIALIZE SESSION STATE ---\n",
    "for key in [\"face_verified\", \"user_name\", \"stored_voice_embedding\"]:\n",
    "    if key not in st.session_state:\n",
    "        st.session_state[key] = None\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "def load_face_encodings():\n",
    "    known_encodings = []\n",
    "    known_names = []\n",
    "    for name in os.listdir(KNOWN_FACES_DIR):\n",
    "        img_path = os.path.join(KNOWN_FACES_DIR, name)\n",
    "        img = face_recognition.load_image_file(img_path)\n",
    "        encodings = face_recognition.face_encodings(img)\n",
    "        if encodings:\n",
    "            known_encodings.append(encodings[0])\n",
    "            known_names.append(os.path.splitext(name)[0])\n",
    "    return known_encodings, known_names\n",
    "\n",
    "def verify_face_live(known_encodings, known_names):\n",
    "    st.markdown(\"### Step 1: Face Verification\")\n",
    "    st.info(\"Please align your face with the camera and click the 'Capture' button.\")\n",
    "    run = st.button(\"Capture\")\n",
    "    if run:\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        time.sleep(2)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "\n",
    "        if not ret:\n",
    "            st.error(\"Failed to capture image.\")\n",
    "            return None\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        face_locations = face_recognition.face_locations(rgb_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "        if len(face_encodings) == 0:\n",
    "            st.warning(\"No face detected.\")\n",
    "            return None\n",
    "\n",
    "        face_encoding = face_encodings[0]\n",
    "        distances = face_recognition.face_distance(known_encodings, face_encoding)\n",
    "        best_match_index = np.argmin(distances)\n",
    "\n",
    "        if distances[best_match_index] < FACE_MATCH_THRESHOLD:\n",
    "            name = known_names[best_match_index]\n",
    "            st.success(f\"Face verified as **{name}**.\")\n",
    "            return name\n",
    "        else:\n",
    "            st.error(\"Face not recognized.\")\n",
    "            return None\n",
    "\n",
    "def load_voice_embedding_for_user(name):\n",
    "    path = os.path.join(VOICE_EMBEDDINGS_DIR, f\"{name}.wav\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    return path\n",
    "\n",
    "def record_voice(duration=4, fs=16000):\n",
    "    st.info(\"Recording for 4 seconds...\")\n",
    "    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "    sd.wait()\n",
    "    file_path = \"temp_user_audio.wav\"\n",
    "    wav.write(file_path, fs, audio)\n",
    "    return file_path\n",
    "\n",
    "def verify_voice(user_audio_path, stored_audio_path):\n",
    "    model = SpeakerRecognition.from_hparams(\n",
    "        source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/voice\"\n",
    "    )\n",
    "    score, _ = model.verify_files(user_audio_path, stored_audio_path)\n",
    "    return (score > VOICE_MATCH_THRESHOLD), score\n",
    "\n",
    "def log_access(user, status, reason=None):\n",
    "    with open(ACCESS_LOG_FILE, \"a\") as f:\n",
    "        line = f\"{datetime.now()}, {user}, {status}\"\n",
    "        if reason:\n",
    "            line += f\", {reason}\"\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "# --- MAIN APP UI ---\n",
    "st.title(\"Multi-modal Biometric Access Control\")\n",
    "st.markdown(\"Secure access using **Face + Voice Recognition**.\")\n",
    "\n",
    "known_face_encodings, known_face_names = load_face_encodings()\n",
    "\n",
    "# Step 1: Face Verification\n",
    "if st.session_state.face_verified is None:\n",
    "    if st.button(\"Start Verification\"):\n",
    "        user = verify_face_live(known_face_encodings, known_face_names)\n",
    "        if user:\n",
    "            st.session_state.face_verified = True\n",
    "            st.session_state.user_name = user\n",
    "            voice_path = load_voice_embedding_for_user(user)\n",
    "            if voice_path:\n",
    "                st.session_state.stored_voice_embedding = voice_path\n",
    "            else:\n",
    "                st.warning(\"‚ö†Ô∏è No stored voice embedding found.\")\n",
    "                log_access(user, \"denied\", \"Missing voice embedding\")\n",
    "                st.session_state.face_verified = None  # reset\n",
    "\n",
    "# Step 2: Voice Verification\n",
    "if st.session_state.face_verified and st.session_state.stored_voice_embedding:\n",
    "    st.markdown(\"### Step 2: Voice Verification\")\n",
    "    if st.button(\"Record Voice\"):\n",
    "        user_voice = record_voice()\n",
    "        verified, score = verify_voice(user_voice, st.session_state.stored_voice_embedding)\n",
    "        if verified:\n",
    "            st.success(f\"‚úÖ Voice verified. Cosine similarity: {score:.2f}\")\n",
    "            st.balloons()\n",
    "            log_access(st.session_state.user_name, \"access granted\")\n",
    "\n",
    "            # New Verification Button\n",
    "            if st.button(\"Start New Verification\"):\n",
    "                for key in [\"user_name\", \"stored_voice_embedding\", \"face_verified\"]:\n",
    "                    st.session_state[key] = None\n",
    "                st.rerun()\n",
    "        else:\n",
    "            st.error(f\"‚ùå Voice mismatch. Cosine similarity: {score:.2f}\")\n",
    "            log_access(st.session_state.user_name, \"denied\", \"Voice mismatch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5080a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement subprocess (from versions: none)\n",
      "ERROR: No matching distribution found for subprocess\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a54d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "\n",
    "# Initialize the TTS engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set properties before adding things to say\n",
    "engine.setProperty('rate', 150)    # Speed percent (can go over 100)\n",
    "engine.setProperty('volume', 0.9)  # Volume 0-1\n",
    "\n",
    "# Optionally, set a specific voice (male/female) if available\n",
    "voices = engine.getProperty('voices')\n",
    "# For male voice\n",
    "engine.setProperty('voice', voices[1].id)\n",
    "# For female voice, uncomment the following line\n",
    "# engine.setProperty('voice', voices[1].id)\n",
    "\n",
    "# Define the welcome message\n",
    "welcome_text = \"Authentication successful. Welcome to the conference.\"\n",
    "\n",
    "# Save the speech to a file\n",
    "engine.save_to_file(welcome_text, 'welcome.wav')\n",
    "\n",
    "# Run the speech engine\n",
    "engine.runAndWait()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b92256",
   "metadata": {},
   "source": [
    "##### Video Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ba0c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b970a11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing videos\n",
    "video_folder = '../data/video'\n",
    "\n",
    "# Directory to save extracted frames\n",
    "frame_output_folder = 'frames_output'\n",
    "os.makedirs(frame_output_folder, exist_ok=True)\n",
    "\n",
    "# Directory to save embeddings\n",
    "embeddings_folder = 'embeddings/video_new'\n",
    "os.makedirs(embeddings_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2fa6f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_embeddings_from_video(video_path, video_name):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_count = 0\n",
    "    embeddings = []\n",
    "    frames_to_extract = 10\n",
    "    frame_indices = np.linspace(0, total_frames - 1, frames_to_extract, dtype=int)\n",
    "\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        if not success or frame is None:\n",
    "            break\n",
    "\n",
    "        if frame_count in frame_indices:\n",
    "            # Convert frame to RGB directly in memory\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Detect face locations and embeddings\n",
    "            face_locations = face_recognition.face_locations(rgb_frame)\n",
    "            if face_locations:\n",
    "                face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "                if face_encodings:\n",
    "                    embeddings.append(face_encodings[0])\n",
    "            else:\n",
    "                print(f\"No face detected at frame {frame_count}. Skipping...\")\n",
    "\n",
    "        frame_count += 1\n",
    "        if len(embeddings) >= frames_to_extract:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extracted {len(embeddings)} embeddings from {video_name}.\")\n",
    "\n",
    "    # Save embeddings if any were found\n",
    "    if embeddings:\n",
    "        embeddings_array = np.array(embeddings)\n",
    "        np.save(os.path.join(embeddings_folder, f\"{video_name}_face_embeddings.npy\"), embeddings_array)\n",
    "        print(f\"Embeddings saved for {video_name}.\")\n",
    "    else:\n",
    "        print(f\"[!] No embeddings found for {video_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae152ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abdullah.mp4\n",
      "aminah.mp4\n",
      "dembele.mp4\n",
      "gentle.mp4\n",
      "haypen.mp4\n",
      "maimunah.mp4\n",
      "meedo.mp4\n",
      "mustapha.mp4\n",
      "rukoyah.mp4\n"
     ]
    }
   ],
   "source": [
    "# Loop through all videos in the folder\n",
    "for filename in os.listdir(video_folder):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2f990f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: abdullah\n",
      "No face detected at frame 613. Skipping...\n",
      "Extracted 9 embeddings from abdullah.\n",
      "Embeddings saved for abdullah.\n",
      "Processing video: aminah\n",
      "Extracted 10 embeddings from aminah.\n",
      "Embeddings saved for aminah.\n",
      "Processing video: dembele\n",
      "No face detected at frame 452. Skipping...\n",
      "Extracted 9 embeddings from dembele.\n",
      "Embeddings saved for dembele.\n",
      "Processing video: gentle\n",
      "No face detected at frame 0. Skipping...\n",
      "No face detected at frame 100. Skipping...\n",
      "No face detected at frame 200. Skipping...\n",
      "No face detected at frame 301. Skipping...\n",
      "No face detected at frame 502. Skipping...\n",
      "No face detected at frame 602. Skipping...\n",
      "Extracted 4 embeddings from gentle.\n",
      "Embeddings saved for gentle.\n",
      "Processing video: haypen\n",
      "No face detected at frame 1427. Skipping...\n",
      "Extracted 9 embeddings from haypen.\n",
      "Embeddings saved for haypen.\n",
      "Processing video: maimunah\n",
      "Extracted 10 embeddings from maimunah.\n",
      "Embeddings saved for maimunah.\n",
      "Processing video: meedo\n",
      "No face detected at frame 55. Skipping...\n",
      "No face detected at frame 74. Skipping...\n",
      "No face detected at frame 111. Skipping...\n",
      "No face detected at frame 129. Skipping...\n",
      "Extracted 6 embeddings from meedo.\n",
      "Embeddings saved for meedo.\n",
      "Processing video: mustapha\n",
      "Extracted 10 embeddings from mustapha.\n",
      "Embeddings saved for mustapha.\n",
      "Processing video: rukoyah\n",
      "No face detected at frame 132. Skipping...\n",
      "Extracted 9 embeddings from rukoyah.\n",
      "Embeddings saved for rukoyah.\n"
     ]
    }
   ],
   "source": [
    "# Loop through all videos in the folder\n",
    "for filename in os.listdir(video_folder):\n",
    "    if filename.endswith('.mp4'):  # or any other video format you have\n",
    "        video_path = os.path.join(video_folder, filename)\n",
    "        video_name = os.path.splitext(filename)[0]  # Remove extension\n",
    "        print(f\"Processing video: {video_name}\")\n",
    "        extract_face_embeddings_from_video(video_path, video_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c503e297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cd20b33",
   "metadata": {},
   "source": [
    "##### Audio Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d071000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Portfolio\\GitHub\\biometric_access_control\\venv310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.transforms import Resample\n",
    "from speechbrain.inference.speaker import SpeakerRecognition\n",
    "from torchaudio.functional import vad\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da13e940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "AUDIO_DIR = '../data/audio'  # Directory where your audio files are located\n",
    "EMBEDDING_DIR = 'embeddings/audio_new'  # Directory to save embeddings\n",
    "MODEL_PATH = os.path.join(\"pretrained_models\", \"spkrec\")  # Path to the pre-trained model\n",
    "\n",
    "# Ensure the embedding directory exists\n",
    "os.makedirs(EMBEDDING_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb4d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "BATCH_SIZE = 4\n",
    "TARGET_SR = 16000  # Required by most models\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3def6f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Portfolio\\GitHub\\biometric_access_control\\venv310\\lib\\site-packages\\speechbrain\\utils\\autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "c:\\Users\\USER\\Portfolio\\GitHub\\biometric_access_control\\venv310\\lib\\site-packages\\speechbrain\\utils\\parameter_transfer.py:234: UserWarning: Requested Pretrainer collection using symlinks on Windows. This might not work; see `LocalStrategy` documentation. Consider unsetting `collect_in` in Pretrainer to avoid symlinking altogether.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# === Load Model ===\n",
    "speaker_model = SpeakerRecognition.from_hparams(\n",
    "    source=MODEL_PATH,\n",
    "    savedir=MODEL_PATH,\n",
    "    run_opts={\"device\": DEVICE},\n",
    "    use_auth_token=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86cf6527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import Vad\n",
    "\n",
    "# Define global VAD transform\n",
    "vad_transform = Vad(sample_rate=16000)\n",
    "\n",
    "def apply_vad(waveform, sample_rate):\n",
    "    if sample_rate != 16000:\n",
    "        waveform = Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "    return vad_transform(waveform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56862a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Preprocess Audio (resample, VAD) ===\n",
    "def preprocess_audio(file_path):\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "\n",
    "    # Convert stereo to mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Apply VAD to remove silence\n",
    "    waveform = apply_vad(waveform, sr)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != TARGET_SR:\n",
    "        waveform = Resample(sr, TARGET_SR)(waveform)\n",
    "\n",
    "    return waveform.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a90aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to save audio embedding\n",
    "# def save_audio_embedding(name, embedding):\n",
    "#     embedding_filename = os.path.join(EMBEDDING_DIR, f\"{name}_voice.pt\")\n",
    "#     torch.save(embedding, embedding_filename)  # Save the embedding\n",
    "#     print(f\"Embedding saved for {name} as {embedding_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c5d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def l2_normalize(embedding):\n",
    "    return F.normalize(embedding, p=2, dim=0)\n",
    "\n",
    "# === Extract embedding ===\n",
    "def extract_embedding(waveform):\n",
    "    embedding = speaker_model.encode_batch(waveform.unsqueeze(0)).squeeze().detach()\n",
    "    embedding = l2_normalize(embedding)\n",
    "    return embedding\n",
    "\n",
    "# === Save embedding ===\n",
    "def save_embedding(name, embedding):\n",
    "    file_path = os.path.join(EMBEDDING_DIR, f\"{name}_voice.pt\")\n",
    "    torch.save(embedding, file_path)\n",
    "    print(f\"‚úÖ Saved: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fde9f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found 9 audio files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: embeddings/audio_new\\abdullah_voice.pt\n",
      "‚úÖ Saved: embeddings/audio_new\\aminah_voice.pt\n",
      "‚úÖ Saved: embeddings/audio_new\\dembele_voice.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:17<00:35, 17.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: embeddings/audio_new\\gentle_voice.pt\n",
      "‚úÖ Saved: embeddings/audio_new\\haypen_voice.pt\n",
      "[!] Failed for maimunah: audio too short or noisy\n",
      "Audio has very low volume or silence .\n",
      "Possible reasons: microphone off, silence, or heavy noise cancellation.\n",
      "‚úîÔ∏è Please re-record with clear speech and minimal background noise.\n",
      "[!] Failed for meedo: audio too short or noisy\n",
      "Audio has very low volume or silence .\n",
      "Possible reasons: microphone off, silence, or heavy noise cancellation.\n",
      "‚úîÔ∏è Please re-record with clear speech and minimal background noise.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:26<00:12, 12.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: embeddings/audio_new\\mustapha_voice.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:29<00:00,  9.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: embeddings/audio_new\\rukoyah_voice.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "audio_files = [\n",
    "    f for f in os.listdir(AUDIO_DIR)\n",
    "    if f.endswith('.wav')\n",
    "]\n",
    "\n",
    "print(f\"üìÇ Found {len(audio_files)} audio files.\")\n",
    "\n",
    "for i in tqdm(range(0, len(audio_files), BATCH_SIZE)):\n",
    "    batch_files = audio_files[i:i + BATCH_SIZE]\n",
    "    for filename in batch_files:\n",
    "        name = os.path.splitext(filename)[0]\n",
    "        path = os.path.join(AUDIO_DIR, filename)\n",
    "\n",
    "        try:\n",
    "            waveform = preprocess_audio(path)\n",
    "            embedding = extract_embedding(waveform)\n",
    "            save_embedding(name, embedding)\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Failed for {name}: audio too short or noisy\")\n",
    "            if \"size should be less than the corresponding input dimension\" in str(e).lower():\n",
    "                print(f\"Audio has very low volume or silence .\\n\"\n",
    "            f\"Possible reasons: microphone off, silence, or heavy noise cancellation.\\n\"\n",
    "            f\"‚úîÔ∏è Please re-record with clear speech and minimal background noise.\"  \n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3b7cbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/audio\\rukoyah.wav\n"
     ]
    }
   ],
   "source": [
    "for filename in batch_files:\n",
    "    name = os.path.splitext(filename)[0]\n",
    "    path = os.path.join(AUDIO_DIR, filename)\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a140ac54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0014)\n"
     ]
    }
   ],
   "source": [
    "waveform, _ = torchaudio.load('..\\data\\\\audio\\maimunah.wav')\n",
    "print(waveform.abs().mean())  # If it's near zero ‚Üí silent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e8c59fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0294)\n"
     ]
    }
   ],
   "source": [
    "waveform, _ = torchaudio.load('mustapha.wav')\n",
    "print(waveform.abs().mean())  # If it's near zero ‚Üí silent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfdc511",
   "metadata": {},
   "source": [
    "##### Audio Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86fe002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import queue\n",
    "import sounddevice as sd\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import scipy.io.wavfile as wav\n",
    "from scipy.io.wavfile import write\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from datetime import datetime\n",
    "from speechbrain.inference.speaker import SpeakerRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d6ab781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Constants ---\n",
    "FACE_EMBED_DIR = \"embeddings/video_new\"\n",
    "VOICE_EMBED_DIR = \"embeddings/audio_new\"\n",
    "MODEL_PATH = os.path.join(\"pretrained_models\", \"spkrec\")\n",
    "AUDIO_TMP_PATH = \"temp_user_audio.wav\"\n",
    "LOG_FILE = \"access_logs.log\"\n",
    "FACE_MATCH_THRESHOLD = 0.4\n",
    "VOICE_MATCH_THRESHOLD = 0.60\n",
    "RECORD_DURATION = 10\n",
    "SAMPLE_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2baa05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "BATCH_SIZE = 4\n",
    "TARGET_SR = 16000  # Required by most models\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f55e2c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'streamlit' has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# === Load Model ===\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m speaker_model \u001b[38;5;241m=\u001b[39m \u001b[43mSpeakerRecognition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_hparams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msavedir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_opts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\Portfolio\\GitHub\\biometric_access_control\\venv310\\lib\\site-packages\\speechbrain\\inference\\interfaces.py:477\u001b[0m, in \u001b[0;36mPretrained.from_hparams\u001b[1;34m(cls, source, hparams_file, pymodule_file, overrides, savedir, use_auth_token, revision, download_only, huggingface_cache_dir, overrides_must_match, local_strategy, **kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_hparams\u001b[39m(\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    416\u001b[0m ):\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fetch and load based from outside source based on HyperPyYAML file\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m    The source can be a location on the filesystem or online/huggingface\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;124;03m    Instance of cls\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m     hparams_local_path \u001b[38;5;241m=\u001b[39m \u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43msavedir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msavedir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhuggingface_cache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhuggingface_cache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m         pymodule_local_path \u001b[38;5;241m=\u001b[39m fetch(\n\u001b[0;32m    490\u001b[0m             filename\u001b[38;5;241m=\u001b[39mpymodule_file,\n\u001b[0;32m    491\u001b[0m             source\u001b[38;5;241m=\u001b[39msource,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    498\u001b[0m             local_strategy\u001b[38;5;241m=\u001b[39mlocal_strategy,\n\u001b[0;32m    499\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\USER\\Portfolio\\GitHub\\biometric_access_control\\venv310\\lib\\site-packages\\speechbrain\\utils\\fetching.py:323\u001b[0m, in \u001b[0;36mfetch\u001b[1;34m(filename, source, savedir, overwrite, allow_updates, allow_network, save_filename, use_auth_token, revision, huggingface_cache_dir, local_strategy)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    318\u001b[0m     destination \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m destination\u001b[38;5;241m.\u001b[39mexists()\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_try_update\n\u001b[0;32m    321\u001b[0m ):\n\u001b[0;32m    322\u001b[0m     file_kind \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymlink\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m destination\u001b[38;5;241m.\u001b[39mis_symlink() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 323\u001b[0m     \u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFetch \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m: Using \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m found at \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_kind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m destination\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fetch_from \u001b[38;5;241m==\u001b[39m FetchFrom\u001b[38;5;241m.\u001b[39mLOCAL:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\logging\\__init__.py:1841\u001b[0m, in \u001b[0;36mLoggerAdapter.info\u001b[1;34m(self, msg, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1837\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minfo\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1838\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1839\u001b[0m \u001b[38;5;124;03m    Delegate an info call to the underlying logger.\u001b[39;00m\n\u001b[0;32m   1840\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1841\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(INFO, msg, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\Portfolio\\GitHub\\biometric_access_control\\venv310\\lib\\site-packages\\speechbrain\\utils\\logger.py:128\u001b[0m, in \u001b[0;36mMultiProcessLoggerAdapter.log\u001b[1;34m(self, level, msg, *args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_log(main_process_only):\n\u001b[0;32m    127\u001b[0m     msg, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess(msg, kwargs)\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog(level, msg, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\logging\\__init__.py:1547\u001b[0m, in \u001b[0;36mLogger.log\u001b[1;34m(self, level, msg, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1545\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misEnabledFor(level):\n\u001b[1;32m-> 1547\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log(level, msg, args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\logging\\__init__.py:1624\u001b[0m, in \u001b[0;36mLogger._log\u001b[1;34m(self, level, msg, args, exc_info, extra, stack_info, stacklevel)\u001b[0m\n\u001b[0;32m   1621\u001b[0m         exc_info \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n\u001b[0;32m   1622\u001b[0m record \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakeRecord(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, level, fn, lno, msg, args,\n\u001b[0;32m   1623\u001b[0m                          exc_info, func, extra, sinfo)\n\u001b[1;32m-> 1624\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\logging\\__init__.py:1634\u001b[0m, in \u001b[0;36mLogger.handle\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m   1627\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m \u001b[38;5;124;03mCall the handlers for the specified record.\u001b[39;00m\n\u001b[0;32m   1629\u001b[0m \n\u001b[0;32m   1630\u001b[0m \u001b[38;5;124;03mThis method is used for unpickled records received from a socket, as\u001b[39;00m\n\u001b[0;32m   1631\u001b[0m \u001b[38;5;124;03mwell as those created locally. Logger-level filtering is applied.\u001b[39;00m\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisabled) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter(record):\n\u001b[1;32m-> 1634\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallHandlers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\logging\\__init__.py:1696\u001b[0m, in \u001b[0;36mLogger.callHandlers\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m   1694\u001b[0m     found \u001b[38;5;241m=\u001b[39m found \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1695\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m record\u001b[38;5;241m.\u001b[39mlevelno \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m hdlr\u001b[38;5;241m.\u001b[39mlevel:\n\u001b[1;32m-> 1696\u001b[0m         \u001b[43mhdlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m c\u001b[38;5;241m.\u001b[39mpropagate:\n\u001b[0;32m   1698\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m    \u001b[38;5;66;03m#break out\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\logging\\__init__.py:968\u001b[0m, in \u001b[0;36mHandler.handle\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\USER\\Portfolio\\GitHub\\biometric_access_control\\venv310\\Scripts\\streamlit.py:31\u001b[0m, in \u001b[0;36mStreamlitLogHandler.emit\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m     29\u001b[0m log_entry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(record)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Show the log entry in Streamlit UI\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43mst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m(log_entry)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'streamlit' has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# === Load Model ===\n",
    "speaker_model = SpeakerRecognition.from_hparams(\n",
    "    source=MODEL_PATH,\n",
    "    savedir=MODEL_PATH,\n",
    "    run_opts={\"device\": DEVICE},\n",
    "    use_auth_token=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be94bf66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927837bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_voice_embedding_for_user(name):\n",
    "    path = os.path.join(VOICE_EMBED_DIR, f\"{name}_voice.pt\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    return path\n",
    "\n",
    "def record_voice(seconds=10, fs=16000):\n",
    "    st.info(\"üé§ Please speak after clicking the record button\")\n",
    "    audio_q = queue.Queue()\n",
    "\n",
    "    def callback(indata, frames, time, status):\n",
    "        audio_q.put(indata.copy())\n",
    "\n",
    "    with sd.InputStream(samplerate=fs, channels=1, callback=callback):\n",
    "        audio_data = []\n",
    "        with st.spinner(\"Recording...\"):\n",
    "            for _ in range(int(fs / 1024 * seconds)):\n",
    "                audio_data.append(audio_q.get())\n",
    "        audio_np = np.concatenate(audio_data, axis=0)\n",
    "        wav.write(AUDIO_TMP_PATH, fs, audio_np)\n",
    "    st.success(\"‚úÖ Recording complete\")\n",
    "    return AUDIO_TMP_PATH\n",
    "\n",
    "def process_audio(file_path):\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "    waveform = waveform.squeeze(0)  # Make sure shape is [time] not [1, time]\n",
    "    embedding = speaker_model.encode_batch(waveform.unsqueeze(0)).squeeze().detach()\n",
    "    return embedding\n",
    "\n",
    "def verify_voice(processed_audio, stored_embedding):\n",
    "    score = F.cosine_similarity(processed_audio, stored_embedding, dim=0).item()\n",
    "    return score > VOICE_MATCH_THRESHOLD, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dc8e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(file_path):\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "    waveform = waveform.squeeze(0)  # Make sure shape is [time] not [1, time]\n",
    "    embedding = speaker_model.encode_batch(waveform.unsqueeze(0)).squeeze().detach()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1668a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_voice(processed_audio, stored_embedding):\n",
    "    score = F.cosine_similarity(processed_audio, stored_embedding, dim=0).item()\n",
    "    return score > VOICE_MATCH_THRESHOLD, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545da18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb6a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3fdd35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97d983",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "\n",
    "def load_voice_embedding_for_user(name):\n",
    "    path = os.path.join(VOICE_EMBEDDINGS_DIR, f\"{name}.wav\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    return path\n",
    "\n",
    "def record_voice(duration=4, fs=16000):\n",
    "    st.info(\"Recording for 4 seconds...\")\n",
    "    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "    sd.wait()\n",
    "    file_path = \"temp_user_audio.wav\"\n",
    "    wav.write(file_path, fs, audio)\n",
    "    return file_path\n",
    "\n",
    "def verify_voice(user_audio_path, stored_audio_path):\n",
    "    model = SpeakerRecognition.from_hparams(\n",
    "        source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/voice\"\n",
    "    )\n",
    "    score, _ = model.verify_files(user_audio_path, stored_audio_path)\n",
    "    return (score > VOICE_MATCH_THRESHOLD), score\n",
    "\n",
    "def log_access(user, status, reason=None):\n",
    "    with open(ACCESS_LOG_FILE, \"a\") as f:\n",
    "        line = f\"{datetime.now()}, {user}, {status}\"\n",
    "        if reason:\n",
    "            line += f\", {reason}\"\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "# --- MAIN APP UI ---\n",
    "st.title(\"Multi-modal Biometric Access Control\")\n",
    "st.markdown(\"Secure access using **Face + Voice Recognition**.\")\n",
    "\n",
    "known_face_encodings, known_face_names = load_face_encodings()\n",
    "\n",
    "# Step 1: Face Verification\n",
    "if st.session_state.face_verified is None:\n",
    "    if st.button(\"Start Verification\"):\n",
    "        user = verify_face_live(known_face_encodings, known_face_names)\n",
    "        if user:\n",
    "            st.session_state.face_verified = True\n",
    "            st.session_state.user_name = user\n",
    "            voice_path = load_voice_embedding_for_user(user)\n",
    "            if voice_path:\n",
    "                st.session_state.stored_voice_embedding = voice_path\n",
    "            else:\n",
    "                st.warning(\"‚ö†Ô∏è No stored voice embedding found.\")\n",
    "                log_access(user, \"denied\", \"Missing voice embedding\")\n",
    "                st.session_state.face_verified = None  # reset\n",
    "\n",
    "# Step 2: Voice Verification\n",
    "if st.session_state.face_verified and st.session_state.stored_voice_embedding:\n",
    "    st.markdown(\"### Step 2: Voice Verification\")\n",
    "    if st.button(\"Record Voice\"):\n",
    "        user_voice = record_voice()\n",
    "        verified, score = verify_voice(user_voice, st.session_state.stored_voice_embedding)\n",
    "        if verified:\n",
    "            st.success(f\"‚úÖ Voice verified. Cosine similarity: {score:.2f}\")\n",
    "            st.balloons()\n",
    "            log_access(st.session_state.user_name, \"access granted\")\n",
    "\n",
    "            # New Verification Button\n",
    "            if st.button(\"Start New Verification\"):\n",
    "                for key in [\"user_name\", \"stored_voice_embedding\", \"face_verified\"]:\n",
    "                    st.session_state[key] = None\n",
    "                st.rerun()\n",
    "        else:\n",
    "            st.error(f\"‚ùå Voice mismatch. Cosine similarity: {score:.2f}\")\n",
    "            log_access(st.session_state.user_name, \"denied\", \"Voice mismatch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a6745e",
   "metadata": {},
   "source": [
    "##### Frames Exraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50e0569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Path to the input video\n",
    "video_dir = '../data/video'\n",
    "\n",
    "# Folder to save extracted frames\n",
    "output_folder = 'frames'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "def extract_frames(video_path, video_name):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames_dir = os.path.join(output_folder, video_name)\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if not success:\n",
    "            print(\"Finished reading the video.\")\n",
    "            break\n",
    "\n",
    "        if frame is None:\n",
    "            print(f\"[!] Frame {frame_count} is None, skipping...\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Convert from BGR (OpenCV default) to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Save frame as image (optional)\n",
    "        frame_filename = os.path.join(frames_dir, f\"{video_name}_{frame_count:04d}.jpg\")\n",
    "        cv2.imwrite(frame_filename, cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR))  # Save back as BGR for normal viewing\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extracted {frame_count} frames.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bbe40cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: abdullah\n",
      "Finished reading the video.\n",
      "Extracted 1106 frames.\n",
      "Processing: aminah\n",
      "Finished reading the video.\n",
      "Extracted 524 frames.\n",
      "Processing: dembele\n",
      "Finished reading the video.\n",
      "Extracted 510 frames.\n",
      "Processing: gentle\n",
      "Finished reading the video.\n",
      "Extracted 905 frames.\n",
      "Processing: haypen\n",
      "Finished reading the video.\n",
      "Extracted 1836 frames.\n",
      "Processing: maimunah\n",
      "Finished reading the video.\n",
      "Extracted 555 frames.\n",
      "Processing: meedo\n",
      "Finished reading the video.\n",
      "Extracted 168 frames.\n",
      "Processing: rukoyah\n",
      "Finished reading the video.\n",
      "Extracted 597 frames.\n"
     ]
    }
   ],
   "source": [
    "# Path to the input video\n",
    "video_dir ='../data/video'\n",
    "video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]  # Get all .wav files\n",
    "for video_filename in video_files:\n",
    "    video_path = os.path.join(video_dir, video_filename)\n",
    "    video_name = os.path.splitext(video_filename)[0]  # Get the base name without extension\n",
    "    print(f\"Processing: {video_name}\")\n",
    "    extract_frames(video_path, video_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d88e1",
   "metadata": {},
   "source": [
    "##### Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436da93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import face_recognition\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d111d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "known_embeddings_dir = \"embeddings/video/\"\n",
    "new_faces_dir = \"new_f/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd475bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_embeddings = []\n",
    "known_names = []\n",
    "\n",
    "for filename in os.listdir(known_embeddings_dir):\n",
    "    if filename.endswith(\".npy\"):\n",
    "        name = os.path.splitext(filename)[0]\n",
    "        embeddings = np.load(os.path.join(known_embeddings_dir, filename))\n",
    "\n",
    "        # Skip if embeddings are empty\n",
    "        if embeddings.ndim != 2 or embeddings.shape[1] != 128:\n",
    "            print(f\"‚ö†Ô∏è Skipping {filename} ‚Äî unexpected shape: {embeddings.shape}\")\n",
    "            continue\n",
    "\n",
    "        # Mean pooling: average over all embeddings\n",
    "        mean_embedding = np.mean(embeddings, axis=0)\n",
    "\n",
    "        known_embeddings.append(mean_embedding)\n",
    "        known_names.append(name)\n",
    "\n",
    "known_embeddings = np.array(known_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afa9c2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 128)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeb189a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: user_01.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\inspect.py:869: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible dimension for X and Y matrices: X.shape[1] == 128 while Y.shape[1] == 192",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m face_encodings:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Compute cosine similarity with each known face\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m encoding\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m     similarities \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknown_embeddings\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     19\u001b[0m     best_match_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(similarities)\n\u001b[0;32m     20\u001b[0m     best_score \u001b[38;5;241m=\u001b[39m similarities[best_match_index]\n",
      "File \u001b[1;32mc:\\Users\\USER\\Portfolio\\GitHub\\biometric_access_control\\venv310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\USER\\Portfolio\\GitHub\\biometric_access_control\\venv310\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1741\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \n\u001b[0;32m   1697\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1737\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1741\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1743\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mc:\\Users\\USER\\Portfolio\\GitHub\\biometric_access_control\\venv310\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:229\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecomputed metric requires shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    223\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(n_queries, n_indexed). Got (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m indexed.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    225\u001b[0m         )\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ensure_2d \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# Only check the number of features if 2d arrays are enforced. Otherwise,\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# validation is left to the user for custom metrics.\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible dimension for X and Y matrices: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m while Y.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    232\u001b[0m     )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, Y\n",
      "\u001b[1;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 128 while Y.shape[1] == 192"
     ]
    }
   ],
   "source": [
    "# Loop through new face images\n",
    "for img_file in os.listdir(new_faces_dir):\n",
    "    if img_file.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "        img_path = os.path.join(new_faces_dir, img_file)\n",
    "        image = face_recognition.load_image_file(img_path)\n",
    "\n",
    "        face_locations = face_recognition.face_locations(image)\n",
    "        face_encodings = face_recognition.face_encodings(image, face_locations)\n",
    "\n",
    "        print(f\"\\nProcessing: {img_file}\")\n",
    "        if len(face_encodings) == 0:\n",
    "            print(\"‚ùå No face found.\")\n",
    "            continue\n",
    "\n",
    "        for encoding in face_encodings:\n",
    "            # Compute cosine similarity with each known face\n",
    "            encoding = encoding.reshape(1, -1)\n",
    "            similarities = cosine_similarity(encoding, known_embeddings)[0]\n",
    "            best_match_index = np.argmax(similarities)\n",
    "            best_score = similarities[best_match_index]\n",
    "\n",
    "            if best_score > 0.6:  # threshold can be adjusted\n",
    "                print(f\"‚úÖ Access Granted: {known_names[best_match_index]} (Score: {best_score:.2f})\")\n",
    "            else:\n",
    "                print(f\"‚ùå Access Denied: Unknown face (Score: {best_score:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de7cf77",
   "metadata": {},
   "source": [
    "##### Voice Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4a6f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Portfolio\\GitHub\\biometric_access_control\\venv310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from speechbrain.inference.speaker import SpeakerRecognition\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43c1a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "EMBEDDING_DIR = 'embeddings/audio'  # Folder containing enrolled .pt files\n",
    "MODEL_PATH = os.path.join(\"pretrained_models\", \"spkrec\")  # Local path to SpeechBrain model\n",
    "AUDIO_PATH = 'new_voices/user01.wav'  # New audio input for recognition\n",
    "\n",
    "# Load speaker recognition model\n",
    "speaker_model = SpeakerRecognition.from_hparams(\n",
    "    source=MODEL_PATH,\n",
    "    savedir=MODEL_PATH,\n",
    "    run_opts={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    use_auth_token=False\n",
    ")\n",
    "\n",
    "# Step 1: Load known voice embeddings\n",
    "known_embeddings = []\n",
    "known_names = []\n",
    "\n",
    "for filename in os.listdir(EMBEDDING_DIR):\n",
    "    if filename.endswith('_voice.pt'):\n",
    "        name = filename.replace('_voice.pt', '')\n",
    "        embedding = torch.load(os.path.join(EMBEDDING_DIR, filename))\n",
    "        known_embeddings.append(embedding)\n",
    "        known_names.append(name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2132dd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found: dembele (score: 0.76)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Process new audio\n",
    "def process_audio(file_path):\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "    waveform = waveform.squeeze(0)\n",
    "    embedding = speaker_model.encode_batch(waveform.unsqueeze(0)).squeeze().detach()\n",
    "    return embedding\n",
    "\n",
    "new_embedding = process_audio(AUDIO_PATH)\n",
    "\n",
    "# Step 3: Compare with known embeddings using cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    return torch.nn.functional.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0)).item()\n",
    "\n",
    "scores = [cosine_similarity(new_embedding, known_emb) for known_emb in known_embeddings]\n",
    "best_index = int(np.argmax(scores))\n",
    "best_score = scores[best_index]\n",
    "\n",
    "# Step 4: Threshold and result\n",
    "threshold = 0.75  # Adjust this based on testing\n",
    "if best_score >= threshold:\n",
    "    print(f\"Match found: {known_names[best_index]} (score: {best_score:.2f})\")\n",
    "else:\n",
    "    print(f\"No match found. Closest match: {known_names[best_index]} (score: {best_score:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd0dfd",
   "metadata": {},
   "source": [
    "##### Realtime Voice Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088ec334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Portfolio\\GitHub\\biometric_access_control\\venv310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "from speechbrain.inference.speaker import SpeakerRecognition\n",
    "import torchaudio\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ec8bcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Portfolio\\GitHub\\biometric_access_control\\venv310\\lib\\site-packages\\speechbrain\\utils\\autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "c:\\Users\\USER\\Portfolio\\GitHub\\biometric_access_control\\venv310\\lib\\site-packages\\speechbrain\\utils\\parameter_transfer.py:234: UserWarning: Requested Pretrainer collection using symlinks on Windows. This might not work; see `LocalStrategy` documentation. Consider unsetting `collect_in` in Pretrainer to avoid symlinking altogether.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "EMBEDDING_DIR = 'embeddings/audio'\n",
    "MODEL_PATH = os.path.join(\"pretrained_models\", \"spkrec\")\n",
    "\n",
    "# Initialize model\n",
    "speaker_model = SpeakerRecognition.from_hparams(\n",
    "    source=MODEL_PATH,\n",
    "    savedir=MODEL_PATH,\n",
    "    run_opts={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    use_auth_token=False\n",
    ")\n",
    "\n",
    "# Load enrolled embeddings\n",
    "known_embeddings = []\n",
    "known_names = []\n",
    "\n",
    "for filename in os.listdir(EMBEDDING_DIR):\n",
    "    if filename.endswith('_voice.pt'):\n",
    "        name = filename.replace('_voice.pt', '')\n",
    "        embedding = torch.load(os.path.join(EMBEDDING_DIR, filename))\n",
    "        known_embeddings.append(embedding)\n",
    "        known_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc8da7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abdullah',\n",
       " 'aminah',\n",
       " 'dembele',\n",
       " 'gentle',\n",
       " 'haypen',\n",
       " 'maimunah',\n",
       " 'meedo',\n",
       " 'rukoyah']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abdullah\n",
      "aminah\n",
      "dembele\n",
      "gentle\n",
      "haypen\n",
      "maimunah\n",
      "meedo\n",
      "rukoyah\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(EMBEDDING_DIR):\n",
    "    if filename.endswith('_voice.pt'):\n",
    "        name = filename.replace('_voice.pt', '')\n",
    "        if name == verified_user:\n",
    "            print(name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d262ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record voice\n",
    "def record_voice(filename='temp.wav', duration=15, fs=16000):\n",
    "    print(f\"\\nüé§ Recording for {duration} seconds...\")\n",
    "    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')\n",
    "    sd.wait()\n",
    "    write(filename, fs, audio)\n",
    "    print(\"‚úÖ Recording complete.\")\n",
    "    return filename\n",
    "\n",
    "# Process audio to embedding\n",
    "def process_audio(file_path):\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "    waveform = waveform.squeeze(0)\n",
    "    embedding = speaker_model.encode_batch(waveform.unsqueeze(0)).squeeze().detach()\n",
    "    return embedding\n",
    "\n",
    "# Cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    return torch.nn.functional.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ca736c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main logic\n",
    "def recognize_from_microphone():\n",
    "    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmpfile:\n",
    "        wav_path = record_voice(tmpfile.name)\n",
    "    \n",
    "    new_embedding = process_audio(wav_path)\n",
    "    scores = [cosine_similarity(new_embedding, emb) for emb in known_embeddings]\n",
    "    \n",
    "    best_index = int(np.argmax(scores))\n",
    "    best_score = scores[best_index]\n",
    "\n",
    "    threshold = 0.75\n",
    "    if best_score >= threshold:\n",
    "        print(f\"\\n‚úÖ Match: {known_names[best_index]} (score: {best_score:.2f})\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Unknown speaker. Closest match: {known_names[best_index]} (score: {best_score:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff704d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé§ Recording for 15 seconds...\n",
      "‚úÖ Recording complete.\n",
      "\n",
      "‚úÖ Match: aminah (score: 0.61)\n"
     ]
    }
   ],
   "source": [
    "recognize_from_microphone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619b8c85",
   "metadata": {},
   "source": [
    "##### Testing Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38df5b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Open the webcam (0 = default camera)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if not video_capture.isOpened():\n",
    "    print(\"[ERROR] Could not open webcam.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9770d63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Press 'q' to stop recording...\n",
      "[ERROR] Failed to read frame.\n"
     ]
    }
   ],
   "source": [
    "# Set resolution (optional)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "# Define the codec and create VideoWriter object for .mp4\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 'mp4v' for .mp4\n",
    "out = cv2.VideoWriter('output.mp4', fourcc, 20.0, (640, 480))\n",
    "\n",
    "print(\"[INFO] Press 'q' to stop recording...\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        print(\"[ERROR] Failed to read frame.\")\n",
    "        break\n",
    "\n",
    "    # Write the frame to output file\n",
    "    out.write(frame)\n",
    "\n",
    "    # Display the resulting frame (can remove if you want speed)\n",
    "    cv2.imshow('Recording...', frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"[INFO] Stopping capture...\")\n",
    "        break\n",
    "\n",
    "# Release everything\n",
    "video_capture.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d18926",
   "metadata": {},
   "source": [
    "##### Realtime Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bd026bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c0d096f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 8 known face embeddings.\n"
     ]
    }
   ],
   "source": [
    "# === Load known face encodings ===\n",
    "known_embeddings_dir = \"embeddings/video\"\n",
    "known_encodings = []\n",
    "known_names = []\n",
    "\n",
    "for filename in os.listdir(known_embeddings_dir):\n",
    "    if filename.endswith(\".npy\"):\n",
    "        name = os.path.splitext(filename)[0]\n",
    "        embedding = np.load(os.path.join(known_embeddings_dir, filename))\n",
    "        known_encodings.append(embedding)\n",
    "        known_names.append(name)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(known_encodings)} known face embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99c2a2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting real-time recognition. Press 'Ctrl+C' to stop.\n",
      "[!] Failed to grab frame.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Start webcam capture ===\n",
    "video_capture = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "if not video_capture.isOpened():\n",
    "    print(\"[ERROR] Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# === Prepare video writer to save the capture ===\n",
    "frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = 10  # You can adjust this\n",
    "\n",
    "# Use mp4v codec for .mp4\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(\"output.mp4\", fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "print(\"[INFO] Starting real-time recognition. Press 'Ctrl+C' to stop.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            print(\"[!] Failed to grab frame.\")\n",
    "            break\n",
    "\n",
    "        # Save the frame to the video file\n",
    "        out.write(frame)\n",
    "\n",
    "        # Resize and convert to RGB for processing\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "        rgb_small_frame = small_frame[:, :, ::-1]\n",
    "\n",
    "        # Detect face locations and encodings\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        for face_encoding in face_encodings:\n",
    "            name = \"Unknown\"\n",
    "            best_score = float('inf')\n",
    "            best_match = None\n",
    "\n",
    "            for known_embedding, known_name in zip(known_encodings, known_names):\n",
    "                distances = face_recognition.face_distance(known_embedding, face_encoding)\n",
    "                min_distance = np.min(distances)\n",
    "                if min_distance < best_score:\n",
    "                    best_score = min_distance\n",
    "                    best_match = known_name\n",
    "\n",
    "            if best_score < 0.6:\n",
    "                print(f\"‚úÖ Face recognized: {best_match}\")\n",
    "            else:\n",
    "                print(\"‚ùå Unknown face detected\")\n",
    "\n",
    "        time.sleep(0.5)  # Control recognition speed\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n[INFO] Recognition stopped by user.\")\n",
    "\n",
    "# === Cleanup ===\n",
    "video_capture.release()\n",
    "out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ed9d8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting real-time recognition. Press 'Ctrl+C' to stop.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# === Start webcam capture ===\n",
    "video_capture = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "if not video_capture.isOpened():\n",
    "    print(\"[ERROR] Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "print(\"[INFO] Starting real-time recognition. Press 'Ctrl+C' to stop.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6249d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to 'output.avi' using MJPG codec at 20 FPS, 640x480 resolution\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('outputww.mp4', fourcc, 20.0, (640, 480))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acb19c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Failed to grab frame.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            print(\"[!] Failed to grab frame.\")\n",
    "            break\n",
    "\n",
    "        # Resize and convert to RGB for processing\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "        rgb_small_frame = small_frame[:, :, ::-1]\n",
    "\n",
    "        # Detect face locations and encodings\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        for face_encoding in face_encodings:\n",
    "            name = \"Unknown\"\n",
    "            best_score = float('inf')\n",
    "            best_match = None\n",
    "\n",
    "            for known_embedding, known_name in zip(known_encodings, known_names):\n",
    "                distances = face_recognition.face_distance(known_embedding, face_encoding)\n",
    "                min_distance = np.min(distances)\n",
    "                if min_distance < best_score:\n",
    "                    best_score = min_distance\n",
    "                    best_match = known_name\n",
    "\n",
    "            # Threshold for recognition\n",
    "            if best_score < 0.6:\n",
    "                print(f\"‚úÖ Face recognized: {best_match}\")\n",
    "            else:\n",
    "                print(\"‚ùå Unknown face detected\")\n",
    "\n",
    "        time.sleep(0.5)  # Adjust as needed\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n[INFO] Recognition stopped by user.\")\n",
    "\n",
    "video_capture.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44d585e",
   "metadata": {},
   "source": [
    "##### Annotated Recognition Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c621666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import face_recognition\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Paths\n",
    "# known_embeddings_dir = \"embeddings/video\"\n",
    "# new_faces_dir = \"new_faces/\"\n",
    "\n",
    "# # Load known face embeddings\n",
    "# known_embeddings = []\n",
    "# known_names = []\n",
    "\n",
    "# for filename in os.listdir(known_embeddings_dir):\n",
    "#     if filename.endswith(\".npy\"):\n",
    "#         name = os.path.splitext(filename)[0]\n",
    "#         embeddings = np.load(os.path.join(known_embeddings_dir, filename))\n",
    "\n",
    "#         if embeddings.ndim != 2 or embeddings.shape[1] != 128:\n",
    "#             print(f\"‚ö†Ô∏è Skipping {filename} ‚Äî unexpected shape: {embeddings.shape}\")\n",
    "#             continue\n",
    "\n",
    "#         mean_embedding = np.mean(embeddings, axis=0)\n",
    "#         known_embeddings.append(mean_embedding)\n",
    "#         known_names.append(name)\n",
    "\n",
    "# known_embeddings = np.array(known_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039e144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process images in new_faces directory\n",
    "# for img_file in os.listdir(new_faces_dir):\n",
    "#     if img_file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "#         img_path = os.path.join(new_faces_dir, img_file)\n",
    "#         image = face_recognition.load_image_file(img_path)\n",
    "\n",
    "#         face_locations = face_recognition.face_locations(image)\n",
    "#         face_encodings = face_recognition.face_encodings(image, face_locations)\n",
    "\n",
    "#         # Convert to BGR for OpenCV display\n",
    "#         image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#         for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "#             encoding = face_encoding.reshape(1, -1)\n",
    "#             similarities = cosine_similarity(encoding, known_embeddings)[0]\n",
    "\n",
    "#             best_match_index = np.argmax(similarities)\n",
    "#             best_score = similarities[best_match_index]\n",
    "\n",
    "#             threshold = 0.5  # cosine similarity threshold\n",
    "#             if best_score >= threshold:\n",
    "#                 name = known_names[best_match_index]\n",
    "#             else:\n",
    "#                 name = \"Unknown\"\n",
    "\n",
    "#             # Draw box and label\n",
    "#             cv2.rectangle(image_bgr, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "#             cv2.putText(image_bgr, name, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "#         # Show the annotated image\n",
    "#         cv2.imshow(\"Recognition\", image_bgr)\n",
    "#         cv2.waitKey(0)\n",
    "\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a022a19",
   "metadata": {},
   "source": [
    "##### Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69e3b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(raw_name):\n",
    "    name = raw_name.replace('_face_embeddings', '').replace('_voice', '')\n",
    "    return name.replace('_', ' ').title()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f3ba4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nhdfhjhhhfgfjhd'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_name('nhdfhjhHHFGfjhd').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9eedf669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nae'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Nae'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a64b5ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mgahegiih'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'Mgahegiih'\n",
    "a.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c69ab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gentle\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(EMBEDDING_DIR):\n",
    "    if filename.endswith('_voice.pt'):\n",
    "        name = filename.replace('_voice.pt', '')\n",
    "        if name == 'gentle':\n",
    "            embedding = torch.load(os.path.join(EMBEDDING_DIR, filename))\n",
    "            print(name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb267018",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_voice_embeddings = []\n",
    "known_voice_names = []\n",
    "\n",
    "for filename in os.listdir(EMBEDDING_DIR):\n",
    "    if filename.endswith('_voice.pt'):\n",
    "        name = filename.replace('_voice.pt', '')\n",
    "        if name == 'gentle':\n",
    "            embedding = torch.load(os.path.join(EMBEDDING_DIR, filename))\n",
    "            known_voice_embeddings.append(embedding)\n",
    "            known_voice_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60e2ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_voice_embeddings = None\n",
    "known_voice_names = []\n",
    "\n",
    "for filename in os.listdir(EMBEDDING_DIR):\n",
    "    if filename.endswith('_voice.pt'):\n",
    "        name = filename.replace('_voice.pt', '')\n",
    "        embedding = torch.load(os.path.join(EMBEDDING_DIR, filename))\n",
    "        known_voice_embeddings = embedding\n",
    "        known_voice_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aeb87997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(known_voice_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e635a7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(known_voice_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0acc4109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.3625e+00,  9.8698e+00,  4.0903e+01,  3.8804e+00,  2.4257e+01,\n",
      "         2.9141e+01,  5.5740e+01,  4.1073e+01, -7.5835e+00,  5.2313e+00,\n",
      "         2.4843e+01,  2.2655e+01,  4.4369e+01,  2.0786e+01,  5.7986e+00,\n",
      "        -3.8210e+01, -1.2403e+01,  2.1943e+01,  1.8472e+01,  1.7310e+01,\n",
      "        -4.5351e+01, -2.6754e+01, -2.8166e+00, -6.5284e+00,  4.3536e+01,\n",
      "        -1.1374e+01, -1.5355e+01,  1.7753e+01,  1.7495e+01, -5.2646e+01,\n",
      "        -2.8624e+01,  1.2043e+01,  1.1832e+01,  4.1213e+01, -2.0512e+01,\n",
      "         2.4462e+01,  1.0415e+01,  5.1659e+00, -2.0074e+01, -1.7107e+00,\n",
      "        -1.7746e+01, -3.1406e+00, -3.9071e+01, -2.1866e+01,  1.5681e+01,\n",
      "         9.5509e-01,  6.3962e+00, -5.5488e+00, -7.3807e+00,  7.1852e+00,\n",
      "         2.4002e+01, -4.0397e+01, -1.2042e+01,  3.8726e+01, -2.6279e+01,\n",
      "         1.4765e+01,  4.1586e+00,  2.5375e+01,  8.0072e+00,  1.9197e+01,\n",
      "        -2.0759e+01,  3.0002e+01, -1.6630e+01, -3.3087e+01,  5.6518e+01,\n",
      "        -1.0584e+01, -1.7382e+01, -7.3311e+00, -4.5305e+01, -2.1345e+01,\n",
      "        -4.5771e+01, -1.0275e+01, -1.1595e+01, -3.9014e+01,  2.8757e+01,\n",
      "        -1.8306e+01,  2.4969e+00, -9.7547e+00,  1.9874e+01,  2.6474e+00,\n",
      "         2.5370e+01,  2.0501e+01, -4.9888e+00,  1.0633e+01, -1.5467e+01,\n",
      "        -2.0415e+01, -1.1620e+00,  2.4528e+01, -3.2313e+00, -2.3220e+01,\n",
      "        -5.3084e+00, -1.5049e+01,  1.8450e+01,  1.0086e+01,  1.7893e+01,\n",
      "         1.4293e+01, -2.0874e+01, -1.3265e+01, -4.4986e+00,  1.8860e+00,\n",
      "         6.6089e+00,  2.4543e+00,  1.1126e+01, -3.2442e+01, -1.2682e+01,\n",
      "         3.5594e+01, -1.9143e+01,  2.6265e+01,  3.6099e+01,  1.4310e+01,\n",
      "         1.7078e+01, -1.7483e+00, -1.9282e+01,  1.3510e+01, -3.3987e+01,\n",
      "        -1.5871e+01,  1.6178e+01,  2.4779e+00,  7.1988e+00, -8.0516e+00,\n",
      "        -2.4653e+01,  1.7287e+01, -1.6656e+01,  7.1365e+00,  2.3362e+01,\n",
      "        -4.0504e+00, -5.5203e+01, -3.3714e+01,  1.6074e+00, -4.5586e+01,\n",
      "        -2.8888e+01,  1.6704e+01,  2.7399e+00,  1.0726e+01, -2.3068e+01,\n",
      "         1.2304e+01, -3.9686e+01, -1.2647e+01,  9.1208e+00, -3.0143e+01,\n",
      "        -9.6539e+00,  1.9609e+00, -1.6225e+01, -3.5093e+01,  2.7029e+01,\n",
      "         2.1563e+01,  3.1700e+00,  1.7465e+01,  1.4970e+01,  2.3772e+01,\n",
      "        -3.1707e-02,  3.8883e+01, -3.0296e+01,  3.6211e+01,  7.6969e+00,\n",
      "        -1.3859e+01,  1.7737e+01,  9.9172e+00, -1.4274e+01, -3.9576e+00,\n",
      "         1.9010e+01,  1.4161e+01,  4.4900e+01, -1.5051e+01,  2.0902e+01,\n",
      "        -2.0971e+01,  1.9922e+01,  1.9401e+01, -1.2539e+01, -2.3190e+01,\n",
      "         7.4409e+00, -5.3152e+00,  4.8558e+01,  1.3307e+01,  3.5443e+00,\n",
      "        -9.2497e+00, -4.3888e+01,  6.8396e+00, -3.8654e+01,  1.9721e+01,\n",
      "         4.4958e+00,  2.0077e+01, -1.1267e+01, -5.3769e+00,  1.7455e+01,\n",
      "         2.0467e+01, -3.8856e+01, -1.0084e+01, -2.0461e+01, -6.1818e+00,\n",
      "         1.0488e+00, -3.8329e+00])\n"
     ]
    }
   ],
   "source": [
    "print(known_voice_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b1fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
